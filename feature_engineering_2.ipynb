{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de197ba2-62a1-447b-9a98-c49b2f83c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    "# ans -- The Filter method is one of the common approaches used in feature selection, which is a process of selecting a subset of relevant features or variables from a larger set of available features in a dataset. The goal of feature selection is to improve the performance of a machine learning model by reducing dimensionality, enhancing interpretability, and potentially speeding up the training process.\n",
    "\n",
    "# The Filter method involves evaluating the importance or relevance of each feature independently of the machine learning algorithm that will be used later. It relies on statistical measures or heuristics to rank or score features based on their individual characteristics. These scores are then used to decide which features to keep or discard.\n",
    "\n",
    "# Here's a general overview of how the Filter method works:\n",
    "\n",
    "#1. Feature Ranking/Scoring: In this step, each feature is assigned a score or ranking based on some predefined criteria. Common criteria or scoring functions include correlation, mutual information, chi-squared test, variance, and more. For example, features with higher correlation with the target variable or features that exhibit significant differences between classes might receive higher scores.\n",
    "\n",
    "#2. Feature Selection: Once the features are ranked or scored, a threshold is set to determine which features to retain. Features that surpass this threshold are selected, while those below the threshold are discarded.\n",
    "\n",
    "#3. Model Training: The selected subset of features is used to train a machine learning model. Since the features were chosen based on their independent relevance, there's no consideration for the interaction between features at this stage.\n",
    "\n",
    "# Advantages of the Filter method include its simplicity and computational efficiency. It can be applied before any specific machine learning algorithm is chosen, making it a good initial step in the feature selection process. However, it might not capture complex interactions between features, which could affect the performance of certain models.\n",
    "\n",
    "# It's important to note that while the Filter method provides a quick and easy way to perform feature selection, it should be used in conjunction with other methods (such as Wrapper and Embedded methods) for a more comprehensive analysis of feature importance and model performance.\n",
    "\n",
    "# In summary, the Filter method involves ranking or scoring features based on their individual relevance using statistical measures, and then selecting a subset of features based on a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e3a2df-ef18-4bf3-849c-f098527a4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 2 \n",
    "# ans -- The Wrapper method is another approach to feature selection, and it differs from the Filter method in several key ways. While both methods aim to select a subset of relevant features to improve the performance of a machine learning model, they do so using different strategies and considerations.\n",
    "\n",
    "# Here's how the Wrapper method differs from the Filter method:\n",
    "\n",
    "#1. Interaction with the Model:\n",
    " #  - Filter Method: The Filter method evaluates the relevance of features independently of the machine learning algorithm that will be used later. It uses predefined criteria or statistical measures to rank or score features without considering how they interact with the specific model.\n",
    " # - Wrapper Method: The Wrapper method involves using a specific machine learning algorithm as part of the feature selection process. It selects features based on their impact on the performance of the chosen model. The model is trained and evaluated multiple times with different subsets of features to determine which combination yields the best performance.\n",
    "\n",
    "#2. Evaluation Criteria:\n",
    "#   - Filter Method: Filter methods use general statistical measures or heuristics, such as correlation, mutual information, or variance, to score features. These measures are not necessarily tailored to the specific model being used.\n",
    " #  - Wrapper Method: Wrapper methods use the performance of the actual machine learning model as the evaluation criterion. The model's performance on a validation or cross-validation set is used to assess the impact of different subsets of features.\n",
    "\n",
    "#3. Computational Cost:\n",
    " #  - Filter Method: Filter methods are generally computationally efficient since they don't involve training and evaluating the model repeatedly. They can be applied before choosing a specific machine learning algorithm.\n",
    "  # - Wrapper Method: Wrapper methods are more computationally expensive since they require training and evaluating the model multiple times for different feature subsets. This can make them more time-consuming, especially if the dataset or model is complex.\n",
    "\n",
    "#4. Flexibility and Model Specificity:\n",
    " #  - Filter Method: Filter methods are relatively independent of the choice of machine learning algorithm. They can provide a quick initial analysis of feature relevance before deciding on a model.\n",
    "  # - Wrapper Method: Wrapper methods are more closely tied to the choice of the machine learning algorithm. Different algorithms might yield different optimal feature subsets. They are more suited when the focus is on optimizing the performance of a specific model.\n",
    "\n",
    "#5. Potential for Overfitting:\n",
    " #  - Filter Method: Filter methods are less prone to overfitting since they do not directly involve the model's performance. However, they might still select irrelevant features if the chosen statistical measures are not well-suited to the problem.\n",
    "  # - Wrapper Method: Wrapper methods have a higher potential for overfitting, especially if the dataset is small or the evaluation process is not carefully designed. The repeated model training and evaluation can lead to optimization bias.\n",
    "\n",
    "# In summary, the Wrapper method involves iteratively training and evaluating a specific machine learning model with different subsets of features to find the optimal combination for performance, while the Filter method evaluates features independently based on statistical measures. Each method has its strengths and weaknesses, and the choice between them depends on the specific goals, constraints, and characteristics of the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a478d5f9-28d8-4a2e-952b-b085ae8bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 3 \n",
    "# ans-- Embedded feature selection methods incorporate feature selection as an integral part of the model training process. These methods aim to select the most relevant features while the model is being trained, resulting in a more efficient and model-specific feature selection process. Here are some common techniques used in Embedded feature selection methods:\n",
    "\n",
    "# 1. LASSO (Least Absolute Shrinkage and Selection Operator):\n",
    " #  LASSO is a linear regression technique that introduces a penalty term based on the absolute values of the coefficients. This penalty encourages some coefficients to become exactly zero, effectively performing feature selection. Features with zero coefficients are considered irrelevant and are automatically excluded from the model. LASSO is particularly useful when dealing with high-dimensional datasets.\n",
    "\n",
    "# 2. Ridge Regression:\n",
    "  # Ridge regression is similar to LASSO but uses a penalty term based on the square of the coefficients. While it doesn't force coefficients to be exactly zero, it penalizes large coefficients, which can lead to shrinkage and effectively reduce the impact of irrelevant features.\n",
    "\n",
    "#3. Elastic Net:\n",
    " #  Elastic Net combines the LASSO and Ridge regression penalties, allowing for both feature selection and handling multicollinearity (correlation between features). It strikes a balance between the two penalties and can be particularly effective when there are groups of correlated features.\n",
    "\n",
    "#4. Tree-based Methods (Random Forest, Gradient Boosting):\n",
    " #  Decision tree-based algorithms like Random Forest and Gradient Boosting naturally perform feature selection as part of their tree-building process. Features that contribute less to the model's predictive power tend to have lower importance scores and are less likely to be included in the final model. These methods can also provide feature importance rankings that can guide feature selection.\n",
    "\n",
    "#5. Regularized Regression Models (Logistic Regression, Support Vector Machines):\n",
    " #  Regularized variants of regression models, such as regularized logistic regression or support vector machines with regularization (SVM with C-parameter), can also perform embedded feature selection. The regularization term in these models penalizes large coefficients, leading to automatic feature selection.\n",
    "\n",
    "#6. Feature Importance from Gradient-based Models (XGBoost, LightGBM):\n",
    " #  Gradient boosting algorithms like XGBoost and LightGBM can provide feature importance scores based on the gradient boosting process. These scores reflect the contribution of each feature to the model's performance, allowing for the identification of important features.\n",
    "\n",
    "#7. Recursive Feature Elimination (RFE):\n",
    " #  While often associated with Wrapper methods, RFE can also be used in an embedded context. It involves recursively training the model and eliminating the least important feature(s) in each iteration. This process continues until a specified number of features is reached.\n",
    "\n",
    "#8. Embedded Neural Network Techniques:\n",
    " #  Some neural network architectures incorporate automatic feature selection as part of the learning process. For example, sparse autoencoders or networks with dropout layers can lead to the selection of relevant features by encouraging sparsity in the network's weights.\n",
    "\n",
    "#Embedded feature selection methods have the advantage of considering feature relevance in the context of the specific model being trained, potentially leading to more effective feature subsets for improved model performance. However, they may also introduce some complexity and computational cost, especially for iterative methods like RFE. The choice of method depends on the problem, dataset characteristics, and the specific algorithm being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe6a60c-477d-459b-98cb-c88572185d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ques 4 \n",
    "# ans-- While the Filter method for feature selection has its advantages, it also comes with several drawbacks and limitations that should be taken into consideration:\n",
    "\n",
    "#.,>Independence Assumption: The Filter method evaluates features independently of the machine learning model that will be used later. This can lead to suboptimal feature subsets, as it doesn't consider potential interactions between features that could be crucial for the model's performance.\n",
    "\n",
    "#2.>Limited to Feature Relationships: The Filter method relies on general statistical measures or heuristics to assess feature relevance. It might miss complex relationships between features that are important for the target task, especially when those relationships are not well-captured by the chosen measures.\n",
    "\n",
    "#3.>Ignores Model Performance: The main goal of feature selection is to improve the model's performance on a specific task. The Filter method doesn't directly consider how features impact the performance of the model. It might select features that are irrelevant or even detrimental to the final model's predictive power.\n",
    "\n",
    "#4.>Threshold Sensitivity: Setting a threshold to determine which features to keep can be challenging. Choosing the right threshold value is often a subjective decision and can significantly impact the final feature subset. An inappropriate threshold might lead to excluding important features or including irrelevant ones.\n",
    "\n",
    "#5.>Doesn't Adapt to Model: Different machine learning algorithms have different requirements for feature subsets. The Filter method does not take the specific characteristics of the chosen algorithm into account, potentially leading to suboptimal results for that particular algorithm.\n",
    "\n",
    "#6.>Limited to Univariate Analysis: Many Filter methods analyze individual features in isolation, which might not be sufficient for capturing the underlying relationships within the data. Complex interactions and patterns that involve multiple features may be overlooked.\n",
    "\n",
    "#7.>Data Preprocessing Impact: The performance of Filter methods can be influenced by the preprocessing steps applied to the data, such as scaling, normalization, or transformation. If these preprocessing steps are not appropriately chosen, the effectiveness of the feature selection process can be compromised.\n",
    "\n",
    "#8.>Feature Redundancy: Filter methods may not account for redundancy among features, resulting in the selection of highly correlated features that do not provide additional information. This can lead to overrepresentation of certain aspects of the data.\n",
    "\n",
    "#9.>Limited Exploration: The Filter method might not thoroughly explore the entire feature space. It could miss valuable features or combinations of features that might be relevant when considered together.\n",
    "\n",
    "#10.>Domain Knowledge Ignored: The Filter method focuses solely on statistical measures and doesn't take into account domain-specific knowledge or expert insights about the features. Important contextual information could be overlooked.\n",
    "\n",
    "#In summary, while the Filter method offers simplicity and computational efficiency, it lacks the ability to adapt to the specific characteristics of the model and might not capture the complex relationships that can significantly impact model performance. It is advisable to combine the Filter method with other feature selection approaches (such as Wrapper or Embedded methods) to overcome these limitations and make more informed decisions about feature relevance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d91ccd6-bc38-429f-ac58-1af09523d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ques 5 -- \n",
    "# ans -- The choice between using the Filter method or the Wrapper method for feature selection depends on the specific characteristics of the problem, the available resources, and the goals of the analysis. There are situations where the Filter method might be preferred over the Wrapper method:\n",
    "\n",
    "#1.> Large Datasets: When dealing with large datasets, the computational cost of running the Wrapper method (which involves training and evaluating the model multiple times) can be quite high. The Filter method, which is generally computationally efficient, can provide a quick initial analysis of feature relevance without the need for extensive model training.\n",
    "\n",
    "#2.> Exploratory Data Analysis: If you're in the early stages of exploring a dataset and don't yet have a specific machine learning algorithm in mind, the Filter method can help you get a sense of which features might have some initial correlation or relevance to the target variable. It provides a starting point for further investigation.\n",
    "\n",
    "#3.> Quick Insights: If your primary goal is to gain quick insights into potential feature relationships and their importance, the Filter method can offer a rapid way to achieve this. You can identify candidate features that are worth exploring further before committing to a specific modeling approach.\n",
    "\n",
    "#4.> Resource Constraints: If you have limited computational resources or time, the Filter method can be a more feasible option. It allows you to perform feature selection without the need to repeatedly train and evaluate a machine learning model.\n",
    "\n",
    "#5.> Baseline Feature Selection: The Filter method can serve as a baseline or reference for evaluating the performance of more complex feature selection methods, such as the Wrapper method. It can help you determine whether the additional computational cost of the Wrapper method is justified.\n",
    "\n",
    "#6.> Feature Engineering Insights: Filter methods can provide insights into which features have higher correlations or relationships with the target variable. These insights can guide feature engineering efforts by highlighting potentially relevant features that could be further transformed or combined to improve model performance.\n",
    "\n",
    "#7.> Noise Filtering: In situations where there might be noisy or irrelevant features that are easy to identify based on their lack of correlation with the target variable or other statistical measures, the Filter method can help in quickly excluding such features.\n",
    "\n",
    "#8.> Benchmarking and Comparison: If you plan to compare multiple datasets or conduct multiple feature selection experiments, the Filter method can provide a consistent and efficient approach for obtaining feature relevance rankings across different datasets.\n",
    "\n",
    "# It's important to note that while the Filter method has its advantages in certain scenarios, it should not be used in isolation. To make more informed and robust decisions about feature selection, it's often beneficial to combine the insights from the Filter method with other techniques, such as the Wrapper or Embedded methods, to ensure that you're considering both individual feature relevance and their interactions within the context of the chosen machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc13ea56-a399-4128-9574-d686d0faccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 6 \n",
    "# ans -- Choosing the most pertinent attributes for a customer churn predictive model using the Filter Method involves a systematic process of evaluating the relevance of each feature based on statistical measures. Here's a step-by-step guide on how you could approach this using the Filter Method:\n",
    "\n",
    "# 1.> Understand the Problem and Data: Begin by thoroughly understanding the problem of customer churn and the available dataset. Identify the target variable (churn) and the features that might influence customer churn, such as customer demographics, usage patterns, contract details, etc.\n",
    "\n",
    "# 2.>Preprocessing: Preprocess the dataset to handle missing values, outliers, and categorical variables. Normalize or scale numerical features if necessary.\n",
    "\n",
    "# 3.> Feature-Target Relationship: Calculate basic statistical measures for each feature in relation to the target variable (churn). Common measures include:\n",
    "\n",
    "    # Correlation: Calculate the correlation coefficient between each numerical feature and the target variable. Features with higher absolute correlation values might be more relevant.\n",
    "    # ANOVA or Chi-squared Test: If the features are categorical and the target variable is binary (churn or not churn), conduct ANOVA for continuous features or chi-squared tests for categorical features to assess if there are significant differences between churn and non-churn groups.\n",
    "#4.> Ranking or Scoring: Rank or score the features based on the calculated statistical measures. For example, you might create a ranked list of features with their correlation coefficients or p-values from the tests.\n",
    "\n",
    "#5.> Select a Threshold: Choose a threshold value to determine which features to keep. This threshold could be based on domain knowledge, a predefined significance level (e.g., p-value < 0.05), or a certain percentile of the ranking scores.\n",
    "\n",
    "#6.> Feature Selection: Select the features that meet the threshold criteria. These are the most pertinent attributes according to the Filter Method. Exclude features that do not meet the threshold.\n",
    "\n",
    "#7.>Model Training and Validation: Train a preliminary customer churn predictive model using the selected features and a simple algorithm like logistic regression. Evaluate the model's performance using cross-validation or a holdout validation set. This step helps you understand the initial impact of the selected features on model performance.\n",
    "\n",
    "#8.> Iterate and Refine: Based on the model's performance, iterate and refine the feature selection process. You might adjust the threshold, consider interactions between features, or incorporate domain knowledge to improve the model.\n",
    "\n",
    "#9.> Feature Engineering: Use insights gained from the Filter Method to inform feature engineering efforts. Transform or create new features that leverage the relationships identified between selected features and the target variable.\n",
    "\n",
    "#10> Model Tuning and Evaluation: Once you've narrowed down the feature set using the Filter Method, experiment with more complex algorithms and hyperparameter tuning. Evaluate the model's performance on different evaluation metrics, such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "#11.> Interpretation: Interpret the selected features in the context of the churn prediction problem. Understand how these features impact customer behavior and influence the likelihood of churn.\n",
    "\n",
    "# Remember that the Filter Method provides an initial screening of feature relevance but does not account for feature interactions or the specific modeling algorithm's requirements. It's recommended to complement the Filter Method with other techniques, such as Wrapper or Embedded methods, to ensure a comprehensive feature selection process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5d39c6-50e1-487d-a900-cfe3fb74f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 7 \n",
    "# ans -- Using the Embedded method for feature selection in the context of predicting the outcome of a soccer match involves incorporating feature selection directly into the model training process. Embedded methods iteratively train the model while selecting relevant features based on their impact on the model's performance. Here's how you could apply the Embedded method to select the most relevant features for your soccer match prediction model:\n",
    "\n",
    "# 1. **Understand the Problem and Data**: Gain a deep understanding of the soccer match prediction problem and the available dataset. Identify the target variable (match outcome) and the features that might influence the outcome, such as player statistics, team rankings, match venue, historical performance, etc.\n",
    "\n",
    "# 2. **Preprocessing**: Preprocess the dataset to handle missing values, outliers, and categorical variables. Normalize or scale numerical features if necessary. Convert categorical features into numerical representations using techniques like one-hot encoding.\n",
    "\n",
    "# 3. **Select a Model**: Choose a machine learning algorithm suitable for the soccer match prediction task. This could be a classifier like logistic regression, decision trees, random forests, gradient boosting, or even neural networks.\n",
    "\n",
    "# 4. **Initial Feature Set**: Start with an initial set of features that you believe are relevant based on domain knowledge or previous research. This set can include various player and team statistics, historical match results, and other relevant information.\n",
    "\n",
    "# 5. **Feature Importance**: Many machine learning algorithms provide a way to calculate feature importance during or after training. Train the chosen model using the initial feature set and calculate feature importance scores. The importance scores indicate how much each feature contributes to the model's predictive power.\n",
    "\n",
    "# 6. **Feature Selection Criteria**: Based on the calculated feature importance scores, set a threshold to determine which features to keep. You can use techniques like selecting the top N features by importance or selecting features with importance scores above a certain threshold.\n",
    "\n",
    "# 7. **Model Training and Evaluation**: Train the model using the selected features and evaluate its performance on a validation or cross-validation set. Monitor metrics such as accuracy, precision, recall, F1-score, or any other suitable evaluation metrics for soccer match prediction.\n",
    "\n",
    "# 8. **Feature Iteration and Refinement**: If the model's performance is not satisfactory, iterate and refine the feature selection process. Experiment with different thresholds, consider interactions between features, and explore additional data sources if available.\n",
    "\n",
    "# 9. **Regularization**: Some algorithms, like regularized linear models (e.g., Lasso or Ridge regression) and tree-based models, inherently perform feature selection as part of their training process. You can leverage these algorithms to automatically select relevant features while controlling for overfitting.\n",
    "\n",
    "# 10. **Cross-Validation**: Use cross-validation to validate the model's performance and the selected feature set on multiple folds of the data. This helps ensure that the feature selection process is stable and not biased by specific data splits.\n",
    "\n",
    "# 11. **Hyperparameter Tuning**: Fine-tune the hyperparameters of the chosen algorithm to optimize the model's performance further. Hyperparameter tuning can also influence the importance of features and their impact on the model.\n",
    "\n",
    "# 12. **Final Model Evaluation**: Once you're satisfied with the model's performance, evaluate it on a separate test dataset to assess its generalization capability.\n",
    "\n",
    "#  By using the Embedded method, you can iteratively train the model while selecting the most relevant features, leading to a more tailored and effective soccer match prediction model. Keep in mind that the choice of algorithm, hyperparameters, and evaluation metrics will play a significant role in determining the final feature subset and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cf352-ae82-4328-a7e5-c9c40baf10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 8 \n",
    "# ans -- Using the Wrapper method for feature selection in a house price prediction project involves selecting the best set of features by iteratively training and evaluating a predictive model using different subsets of features. This method is more computationally intensive compared to the Filter method but takes into account the specific machine learning algorithm you intend to use. Here's how you could apply the Wrapper method to select the best set of features for your house price predictor:\n",
    "\n",
    "# 1. **Understand the Problem and Data**: Develop a clear understanding of the problem and the dataset. Identify the target variable (house price) and the features that might influence it, such as size, location, age, number of bedrooms, etc.\n",
    "\n",
    "# 2. **Data Preprocessing**: Preprocess the dataset to handle missing values, outliers, and categorical variables. Normalize or scale numerical features if necessary. Encode categorical features using appropriate methods, such as one-hot encoding.\n",
    "\n",
    "# 3. **Select a Model**: Choose a machine learning algorithm suitable for house price prediction. Regression algorithms like linear regression, decision trees, random forests, gradient boosting, or even neural networks are common choices.\n",
    "\n",
    "# 4. **Define a Subset of Features**: Start with a small subset of features that you believe are essential for predicting house prices. These features should be selected based on domain knowledge, correlation analysis, or initial research.\n",
    "\n",
    "# 5. **Model Training and Evaluation**: Train the chosen model using the selected subset of features and evaluate its performance on a validation set using a suitable metric, such as mean squared error (MSE) or root mean squared error (RMSE).\n",
    "\n",
    "# 6. **Feature Iteration and Selection**: Begin the wrapper loop by iteratively adding or removing features from the subset. For each iteration:\n",
    "\n",
    " #  a. Train the model using the current subset of features.\n",
    " #  b. Evaluate the model's performance on the validation set using the chosen metric.\n",
    "  # c. Keep track of the model's performance for each subset of features.\n",
    "\n",
    "#7. **Forward or Backward Selection**: You can implement forward selection (starting with a small subset and adding features) or backward elimination (starting with all features and removing them one by one) based on your preference. Forward selection might be a good starting point, especially if you have a limited number of features.\n",
    "\n",
    "# 8. **Cross-Validation**: Perform cross-validation during each iteration to ensure the stability and generalization of the selected feature subset. Cross-validation helps mitigate overfitting and provides a more robust estimate of the model's performance.\n",
    "\n",
    "# 9. **Stopping Criterion**: Decide on a stopping criterion for the wrapper loop. This could be a predefined number of iterations, reaching a plateau in performance improvement, or any other relevant criteria.\n",
    "\n",
    "# 10. **Final Model and Feature Set**: Once the wrapper loop is complete, choose the final feature subset based on the best-performing model. This set of features will be used for the final model training.\n",
    "\n",
    "# 11. **Hyperparameter Tuning**: Fine-tune the hyperparameters of the chosen algorithm using techniques like grid search or random search to optimize the model's performance further.\n",
    "\n",
    "# 12. **Model Evaluation**: Evaluate the final model on a separate test dataset to assess its generalization capability and provide a realistic estimate of its performance.\n",
    "\n",
    "# The Wrapper method allows you to find the best subset of features by considering their interactions within the context of the chosen model. Keep in mind that the Wrapper method can be computationally intensive, especially when dealing with a large number of features. It's important to strike a balance between feature selection and computational resources while ensuring that the selected features make sense in the context of the problem domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
